{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It recently came to my attention that everybody is implementing NALUs (from [this paper](https://arxiv.org/pdf/1808.00508.pdf)) in PyTorch seemingly incorrectly. When you Google \"PyTorch NALU\" all the implementations that show up on the first page (as of the time of writing) contain the same mistake that turns their NACs into simple Linear modules. In this short notebook, I'll go over the incorrect approach and then provide a correct implementation of NAC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import init\n",
    "from torch.nn.modules import Module\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fauxNAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fauxNAC(Module):\n",
    "    def __init__(self, n_in, n_out):\n",
    "        super().__init__()\n",
    "        self.W_hat = Parameter(torch.Tensor(n_out, n_in))\n",
    "        self.M_hat = Parameter(torch.Tensor(n_out, n_in))\n",
    "        self.weights = Parameter(torch.tanh(self.W_hat) * torch.sigmoid(self.M_hat))\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        init.kaiming_uniform_(self.W_hat)\n",
    "        init.kaiming_uniform_(self.M_hat)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return F.linear(input, self.weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is more or less similar to other PyTorch implementations I've seen and is **wrong**. The mistake is in the 8th line of the above cell: `weights` should not be a member variable and a Parameter. Instead it should be calculated in `forward()` using the values of `W_hat` and `M_hat`. The way it's implemented in `fauxNAC` means that `W_hat` and `M_hat` will never get updated and this entire module is just a Linear layer, but with worse memory usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since seeing is believing, I'll show first hand how `W_hat` and `M_hat` don't get updated at all, by training `fauxNAC` on a simple summing learning task. If you've already been convinced and want to see the correct implementation, skip to the end of the notebook or take a look at the rest of this repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sum task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by writing a simple training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(m, dataloader, opt, crit):\n",
    "    for epoch in range(100):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(dataloader):\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.cuda().float()\n",
    "            labels = labels.cuda().float()\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            opt.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = m(inputs)\n",
    "            loss = crit(outputs, labels)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 8 == 7 and epoch % 20 == 19: # Print every eight minibatch of every 20th epoch\n",
    "                print('[%d] loss: %.3f' % (epoch + 1, running_loss / 8))\n",
    "                running_loss = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we setup the training data, model, optimiser, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([np.array([a, b]) for a, b in zip(np.random.uniform(-5, 5, 4096), np.random.uniform(-5, 5, 4096))])\n",
    "y = np.array([np.array([a + b]) for a, b in data])\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(torch.Tensor(data), torch.Tensor(y))\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=512, shuffle=True, num_workers=0)\n",
    "\n",
    "m = fauxNAC(2, 1).cuda()\n",
    "opt = optim.Adam(m.parameters(), 1e-2)\n",
    "crit = F.mse_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the values of our parameters `M_hat` and `W_hat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.3000, -1.3239]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.M_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.7358, -1.4481]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.W_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train and see what happens to those parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20] loss: 0.062\n",
      "[40] loss: 0.000\n",
      "[60] loss: 0.000\n",
      "[80] loss: 0.000\n",
      "[100] loss: 0.000\n"
     ]
    }
   ],
   "source": [
    "fit(m, dataloader, opt, crit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.3000, -1.3239]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.M_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.7358, -1.4481]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.W_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As should be expected, they were not changed. The Parameter that was changed instead was `weights`, making the `fauxNAC` just a linear module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[1.0000, 1.0000]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correct implementation\n",
    "\n",
    "Next, I'll give the correct implementation of NAC by using only `M_hat` and `W_hat` as Parameters and performing the calculation in the `forward()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NAC(Module):\n",
    "    def __init__(self, n_in, n_out):\n",
    "        super().__init__()\n",
    "        self.W_hat = Parameter(torch.Tensor(n_out, n_in))\n",
    "        self.M_hat = Parameter(torch.Tensor(n_out, n_in))\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        init.kaiming_uniform_(self.W_hat)\n",
    "        init.kaiming_uniform_(self.M_hat)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        weights = torch.tanh(self.W_hat) * torch.sigmoid(self.M_hat)\n",
    "        return F.linear(input, weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
